#!/usr/bin/env python3
"""
Phase 1: ç”³è«‹äººç™»éŒ²æƒ…å ±ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
ç”³è«‹äººåå–å¾—ç‡ã‚’14.8%â†’100%ã«æ”¹å–„ã™ã‚‹é‡è¦ãªå®Ÿè£…
"""

import sqlite3
import csv
import sys
from pathlib import Path
from typing import List, Dict
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ApplicantMasterImporter:
    def __init__(self, db_path: str = "output.db"):
        self.db_path = db_path
        self.conn = None
        
    def connect_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š"""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.row_factory = sqlite3.Row
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ: {self.db_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—: {e}")
            return False
    
    def import_applicant_master(self, tsv_file_path: str) -> bool:
        """ç”³è«‹äººç™»éŒ²æƒ…å ±ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        if not Path(tsv_file_path).exists():
            logger.error(f"âŒ TSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {tsv_file_path}")
            return False
        
        try:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢
            cursor = self.conn.cursor()
            cursor.execute("DELETE FROM applicant_master_full")
            logger.info("ğŸ—‘ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
            
            # TSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            with open(tsv_file_path, 'r', encoding='utf-8') as file:
                reader = csv.DictReader(file, delimiter='\t')
                
                records_inserted = 0
                batch_size = 100
                batch_data = []
                
                for row in reader:
                    # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    record = self._normalize_applicant_record(row)
                    batch_data.append(record)
                    
                    if len(batch_data) >= batch_size:
                        self._insert_batch(batch_data, "applicant_master_full")
                        records_inserted += len(batch_data)
                        batch_data = []
                        
                        if records_inserted % 500 == 0:
                            logger.info(f"ğŸ“Š ã‚¤ãƒ³ãƒãƒ¼ãƒˆé€²è¡Œä¸­: {records_inserted}ä»¶")
                
                # æ®‹ã‚Šã®ãƒãƒƒãƒå‡¦ç†
                if batch_data:
                    self._insert_batch(batch_data, "applicant_master_full")
                    records_inserted += len(batch_data)
                
                self.conn.commit()
                logger.info(f"âœ… ç”³è«‹äººãƒã‚¹ã‚¿ãƒ¼ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: {records_inserted}ä»¶")
                
                # FTS5ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
                self._update_fts_index()
                
                return True
                
        except Exception as e:
            logger.error(f"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            if self.conn:
                self.conn.rollback()
            return False
    
    def import_integration_mapping(self, tsv_file_path: str) -> bool:
        """ç”³è«‹äººçµ±åˆãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        if not Path(tsv_file_path).exists():
            logger.error(f"âŒ TSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {tsv_file_path}")
            return False
        
        try:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢
            cursor = self.conn.cursor()
            cursor.execute("DELETE FROM applicant_integration_mapping")
            logger.info("ğŸ—‘ï¸ æ—¢å­˜çµ±åˆãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
            
            # TSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            with open(tsv_file_path, 'r', encoding='utf-8') as file:
                reader = csv.DictReader(file, delimiter='\t')
                
                records_inserted = 0
                batch_data = []
                
                for row in reader:
                    record = {
                        'appl_cd': row.get('appl_cd', '').strip(),
                        'repeat_num': int(row.get('repeat_num', 0)),
                        'under_integ_appl_cd': row.get('under_integ_appl_cd', '').strip()
                    }
                    batch_data.append(record)
                    
                    if len(batch_data) >= 50:
                        self._insert_batch(batch_data, "applicant_integration_mapping")
                        records_inserted += len(batch_data)
                        batch_data = []
                
                # æ®‹ã‚Šã®ãƒãƒƒãƒå‡¦ç†
                if batch_data:
                    self._insert_batch(batch_data, "applicant_integration_mapping")
                    records_inserted += len(batch_data)
                
                self.conn.commit()
                logger.info(f"âœ… çµ±åˆãƒãƒƒãƒ”ãƒ³ã‚°ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: {records_inserted}ä»¶")
                return True
                
        except Exception as e:
            logger.error(f"âŒ çµ±åˆãƒãƒƒãƒ”ãƒ³ã‚°ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            if self.conn:
                self.conn.rollback()
            return False
    
    def _normalize_applicant_record(self, row: Dict) -> Dict:
        """ç”³è«‹äººãƒ¬ã‚³ãƒ¼ãƒ‰ã®æ­£è¦åŒ–ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        # ã€Œï¼ˆçœç•¥ï¼‰ã€ã‚’ç©ºæ–‡å­—ã«ç½®æ›
        def clean_field(value):
            if value and value.strip() == 'ï¼ˆçœç•¥ï¼‰':
                return ''
            return value.strip() if value else ''
        
        return {
            'data_id_cd': row.get('data_id_cd', '').strip(),
            'appl_cd': row.get('appl_cd', '').strip(),
            'appl_name': clean_field(row.get('appl_name', '')),
            'appl_cana_name': clean_field(row.get('appl_cana_name', '')),
            'appl_postcode': clean_field(row.get('appl_postcode', '')),
            'appl_addr': clean_field(row.get('appl_addr', '')),
            'wes_join_name': clean_field(row.get('wes_join_name', '')),
            'wes_join_addr': clean_field(row.get('wes_join_addr', '')),
            'integ_appl_cd': clean_field(row.get('integ_appl_cd', '')),
            'dbl_reg_integ_mgt_srl_num': int(row.get('dbl_reg_integ_mgt_srl_num', 0))
        }
    
    def _insert_batch(self, batch_data: List[Dict], table_name: str):
        """ãƒãƒƒãƒã‚¤ãƒ³ã‚µãƒ¼ãƒˆ"""
        if not batch_data:
            return
        
        cursor = self.conn.cursor()
        
        if table_name == "applicant_master_full":
            sql = """
                INSERT INTO applicant_master_full (
                    data_id_cd, appl_cd, appl_name, appl_cana_name,
                    appl_postcode, appl_addr, wes_join_name, wes_join_addr,
                    integ_appl_cd, dbl_reg_integ_mgt_srl_num
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """
            data_tuples = [
                (r['data_id_cd'], r['appl_cd'], r['appl_name'], r['appl_cana_name'],
                 r['appl_postcode'], r['appl_addr'], r['wes_join_name'], r['wes_join_addr'],
                 r['integ_appl_cd'], r['dbl_reg_integ_mgt_srl_num'])
                for r in batch_data
            ]
        
        elif table_name == "applicant_integration_mapping":
            sql = """
                INSERT INTO applicant_integration_mapping (
                    appl_cd, repeat_num, under_integ_appl_cd
                ) VALUES (?, ?, ?)
            """
            data_tuples = [
                (r['appl_cd'], r['repeat_num'], r['under_integ_appl_cd'])
                for r in batch_data
            ]
        
        cursor.executemany(sql, data_tuples)
    
    def _update_fts_index(self):
        """FTS5å…¨æ–‡æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ›´æ–°"""
        try:
            cursor = self.conn.cursor()
            cursor.execute("DELETE FROM applicant_search_fts")
            cursor.execute("""
                INSERT INTO applicant_search_fts (appl_cd, appl_name, appl_cana_name, appl_addr, wes_join_name)
                SELECT appl_cd, appl_name, appl_cana_name, appl_addr, wes_join_name 
                FROM applicant_master_full
                WHERE appl_name IS NOT NULL AND appl_name != ''
            """)
            self.conn.commit()
            logger.info("âœ… FTS5å…¨æ–‡æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°å®Œäº†")
        except Exception as e:
            logger.warning(f"âš ï¸ FTS5ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def verify_import(self) -> Dict:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆçµæœã®æ¤œè¨¼"""
        cursor = self.conn.cursor()
        
        # åŸºæœ¬çµ±è¨ˆ
        cursor.execute("SELECT COUNT(*) as total FROM applicant_master_full")
        total_count = cursor.fetchone()['total']
        
        cursor.execute("SELECT COUNT(*) as with_name FROM applicant_master_full WHERE appl_name IS NOT NULL AND appl_name != ''")
        with_name_count = cursor.fetchone()['with_name']
        
        cursor.execute("SELECT COUNT(*) as with_addr FROM applicant_master_full WHERE appl_addr IS NOT NULL AND appl_addr != ''")
        with_addr_count = cursor.fetchone()['with_addr']
        
        cursor.execute("SELECT COUNT(*) as with_integration FROM applicant_integration_mapping")
        integration_count = cursor.fetchone()['with_integration']
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
        cursor.execute("SELECT * FROM applicant_master_full WHERE appl_name IS NOT NULL AND appl_name != '' LIMIT 5")
        sample_data = [dict(row) for row in cursor.fetchall()]
        
        results = {
            'total_applicants': total_count,
            'with_name': with_name_count,
            'with_address': with_addr_count,
            'integration_mappings': integration_count,
            'name_coverage_rate': (with_name_count / total_count * 100) if total_count > 0 else 0,
            'sample_data': sample_data
        }
        
        return results
    
    def close(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚º"""
        if self.conn:
            self.conn.close()
            logger.info("ğŸ”’ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ã‚¯ãƒ­ãƒ¼ã‚ºã—ã¾ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ Phase 1: ç”³è«‹äººç™»éŒ²æƒ…å ±ã‚¤ãƒ³ãƒãƒ¼ãƒˆé–‹å§‹")
    
    importer = ApplicantMasterImporter()
    
    if not importer.connect_database():
        sys.exit(1)
    
    try:
        # ç”³è«‹äººç™»éŒ²æƒ…å ±ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        tsv_path_master = "tsv_data/tsv/upd_appl_reg_info.tsv"
        success1 = importer.import_applicant_master(tsv_path_master)
        
        # çµ±åˆãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        tsv_path_integration = "tsv_data/tsv/upd_under_integ_appl_info_mgt.tsv"
        success2 = importer.import_integration_mapping(tsv_path_integration)
        
        if success1 and success2:
            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆçµæœæ¤œè¨¼
            results = importer.verify_import()
            
            logger.info("ğŸ“Š ã‚¤ãƒ³ãƒãƒ¼ãƒˆçµæœ:")
            logger.info(f"   ç·ç”³è«‹äººæ•°: {results['total_applicants']:,}ä»¶")
            logger.info(f"   ç”³è«‹äººåæœ‰ã‚Š: {results['with_name']:,}ä»¶")
            logger.info(f"   ä½æ‰€æœ‰ã‚Š: {results['with_address']:,}ä»¶")
            logger.info(f"   çµ±åˆãƒãƒƒãƒ”ãƒ³ã‚°: {results['integration_mappings']:,}ä»¶")
            logger.info(f"   ç”³è«‹äººåã‚«ãƒãƒ¬ãƒƒã‚¸: {results['name_coverage_rate']:.1f}%")
            
            logger.info("ğŸ¯ Phase 1å®Ÿè£…å®Œäº†ï¼ç”³è«‹äººæƒ…å ±ãŒå¤§å¹…ã«æ”¹å–„ã•ã‚Œã¾ã—ãŸã€‚")
            
        else:
            logger.error("âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
    
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)
    
    finally:
        importer.close()

if __name__ == "__main__":
    main()