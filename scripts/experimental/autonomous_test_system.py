#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ 
å•†æ¨™æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’è‡ªå‹•çš„ã«ãƒ†ã‚¹ãƒˆã—ã€çµæœã®å“è³ªã‚„æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹
"""

import json
import time
import random
import statistics
from pathlib import Path
from typing import List, Dict, Any, Tuple
from datetime import datetime
import sqlite3

from cli_trademark_search import TrademarkSearchCLI

class AutonomousTestSystem:
    """è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, db_path: str = None):
        self.searcher = TrademarkSearchCLI(db_path)
        self.test_results = []
        self.performance_metrics = []
        
    def get_sample_data(self) -> Dict[str, List[str]]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        conn = self.searcher.get_db_connection()
        
        samples = {
            'app_nums': [],
            'mark_texts': [],
            'goods_classes': [],
            'designated_goods': [],
            'similar_group_codes': []
        }
        
        # å‡ºé¡˜ç•ªå·ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå›ºå®šã®3ä»¶ã§é«˜é€ŸåŒ–ï¼‰
        result = self.searcher.query_db(
            "SELECT normalized_app_num FROM jiken_c_t LIMIT 3"
        )
        samples['app_nums'] = [r['normalized_app_num'] for r in result]
        
        # å•†æ¨™æ–‡å­—ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå›ºå®šãƒ‡ãƒ¼ã‚¿ã§é«˜é€ŸåŒ–ï¼‰
        samples['mark_texts'] = ['ã‚½ãƒ‹ãƒ¼', 'ãƒˆãƒ¨ã‚¿', 'ãƒ›ãƒ³ãƒ€']
        
        # å•†å“åŒºåˆ†ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå›ºå®šãƒ‡ãƒ¼ã‚¿ï¼‰
        samples['goods_classes'] = ['9', '35', '42']
        
        # æŒ‡å®šå•†å“ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå›ºå®šãƒ‡ãƒ¼ã‚¿ï¼‰
        samples['designated_goods'] = ['ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿', 'é›»æ°—é€šä¿¡æ©Ÿæ¢°å™¨å…·', 'è‡ªå‹•è»Š']
        
        # é¡ä¼¼ç¾¤ã‚³ãƒ¼ãƒ‰ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå›ºå®šãƒ‡ãƒ¼ã‚¿ï¼‰
        samples['similar_group_codes'] = ['11C01', '35K03', '12A01']
        
        return samples
    
    def create_test_scenarios(self) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã‚’ç”Ÿæˆ"""
        sample_data = self.get_sample_data()
        scenarios = []
        
        # 1. åŸºæœ¬æ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å€‹åˆ¥ - è»½é‡åŒ–ï¼‰
        if sample_data['app_nums']:
            scenarios.append({
                'name': f'å‡ºé¡˜ç•ªå·æ¤œç´¢: {sample_data["app_nums"][0]}',
                'params': {'app_num': sample_data['app_nums'][0]},
                'expected_min_results': 1,
                'expected_max_time': 5.0
            })
        
        scenarios.append({
            'name': 'å•†æ¨™æ–‡å­—æ¤œç´¢: ã‚½ãƒ‹ãƒ¼',
            'params': {'mark_text': 'ã‚½ãƒ‹ãƒ¼', 'limit': 10},
            'expected_min_results': 0,
            'expected_max_time': 10.0
        })
        
        scenarios.append({
            'name': 'å•†å“åŒºåˆ†æ¤œç´¢: 9',
            'params': {'goods_classes': '9', 'limit': 10},
            'expected_min_results': 0,
            'expected_max_time': 10.0
        })
        
        # 2. è¤‡åˆæ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆè»½é‡åŒ–ï¼‰
        scenarios.append({
            'name': 'è¤‡åˆæ¤œç´¢: å•†æ¨™æ–‡å­— + å•†å“åŒºåˆ†',
            'params': {
                'mark_text': 'ã‚½ãƒ‹ãƒ¼',
                'goods_classes': '9',
                'limit': 5
            },
            'expected_min_results': 0,
            'expected_max_time': 15.0
        })
        
        # 3. ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
        scenarios.append({
            'name': 'ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹: å­˜åœ¨ã—ãªã„å‡ºé¡˜ç•ªå·',
            'params': {'app_num': '999999999'},
            'expected_min_results': 0,
            'expected_max_time': 5.0
        })
        
        return scenarios
    
    def run_single_test(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: {scenario['name']}")
        
        start_time = time.time()
        try:
            results, total_count = self.searcher.search_trademarks(**scenario['params'])
            end_time = time.time()
            
            execution_time = end_time - start_time
            
            # çµæœã®è©•ä¾¡
            test_result = {
                'scenario': scenario['name'],
                'params': scenario['params'],
                'success': True,
                'results_count': len(results),
                'total_count': total_count,
                'execution_time': execution_time,
                'timestamp': datetime.now().isoformat(),
                'issues': []
            }
            
            # æœŸå¾…å€¤ãƒã‚§ãƒƒã‚¯
            if len(results) < scenario.get('expected_min_results', 0):
                test_result['issues'].append(f"çµæœæ•°ãŒæœŸå¾…å€¤æœªæº€: {len(results)} < {scenario['expected_min_results']}")
            
            if execution_time > scenario.get('expected_max_time', 30.0):
                test_result['issues'].append(f"å®Ÿè¡Œæ™‚é–“ãŒä¸Šé™è¶…é: {execution_time:.2f}s > {scenario['expected_max_time']}s")
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
            if results:
                quality_issues = self.check_data_quality(results)
                test_result['issues'].extend(quality_issues)
            
            print(f"   âœ… æˆåŠŸ: {len(results)}ä»¶ / {total_count}ä»¶ (å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}s)")
            if test_result['issues']:
                print(f"   âš ï¸  å•é¡Œ: {'; '.join(test_result['issues'])}")
                
        except Exception as e:
            end_time = time.time()
            execution_time = end_time - start_time
            
            test_result = {
                'scenario': scenario['name'],
                'params': scenario['params'],
                'success': False,
                'error': str(e),
                'execution_time': execution_time,
                'timestamp': datetime.now().isoformat(),
                'issues': [f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"]
            }
            
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return test_result
    
    def check_data_quality(self, results: List[Dict[str, Any]]) -> List[str]:
        """çµæœã®ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        if not results:
            return issues
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
        required_fields = ['app_num']
        for i, result in enumerate(results[:5]):  # æœ€åˆã®5ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
            for field in required_fields:
                if not result.get(field):
                    issues.append(f"çµæœ{i+1}: å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ '{field}' ãŒç©º")
        
        # ãƒ‡ãƒ¼ã‚¿ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        app_nums = [r.get('app_num') for r in results if r.get('app_num')]
        if len(set(app_nums)) != len(app_nums):
            issues.append("é‡è¤‡ã—ãŸå‡ºé¡˜ç•ªå·ãŒçµæœã«å«ã¾ã‚Œã‚‹")
        
        # å•†æ¨™è¡¨ç¤ºã®å“è³ªãƒã‚§ãƒƒã‚¯
        no_mark_text_count = sum(1 for r in results if not r.get('mark_text'))
        if no_mark_text_count > len(results) * 0.5:
            issues.append(f"å•†æ¨™æ–‡å­—ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ãªã„çµæœãŒå¤šã„: {no_mark_text_count}/{len(results)}")
        
        return issues
    
    def run_performance_test(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        print("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        # é€£ç¶šæ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆè»½é‡åŒ–ï¼‰
        search_times = []
        for i in range(3):
            start_time = time.time()
            results, total = self.searcher.search_trademarks(mark_text="ã‚½ãƒ‹ãƒ¼", limit=10)
            end_time = time.time()
            search_times.append(end_time - start_time)
            print(f"   æ¤œç´¢{i+1}: {len(results)}ä»¶ ({search_times[-1]:.2f}s)")
        
        # çµ±è¨ˆæƒ…å ±
        avg_time = statistics.mean(search_times)
        max_time = max(search_times)
        min_time = min(search_times)
        
        performance_result = {
            'test_type': 'performance',
            'search_count': len(search_times),
            'average_time': avg_time,
            'max_time': max_time,
            'min_time': min_time,
            'timestamp': datetime.now().isoformat()
        }
        
        print(f"   ğŸ“Š å¹³å‡: {avg_time:.2f}s, æœ€å¤§: {max_time:.2f}s, æœ€å°: {min_time:.2f}s")
        
        return performance_result
    
    def generate_improvement_suggestions(self) -> List[str]:
        """æ”¹å–„ææ¡ˆã®ç”Ÿæˆ"""
        suggestions = []
        
        if not self.test_results:
            return ["ãƒ†ã‚¹ãƒˆçµæœãŒã‚ã‚Šã¾ã›ã‚“"]
        
        # å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®åˆ†æ
        failed_tests = [t for t in self.test_results if not t.get('success', True)]
        if failed_tests:
            suggestions.append(f"å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆ: {len(failed_tests)}ä»¶ - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ”¹å–„ãŒå¿…è¦")
        
        # å®Ÿè¡Œæ™‚é–“ã®åˆ†æ
        slow_tests = [t for t in self.test_results if t.get('execution_time', 0) > 10.0]
        if slow_tests:
            suggestions.append(f"å®Ÿè¡Œæ™‚é–“ãŒé…ã„ãƒ†ã‚¹ãƒˆ: {len(slow_tests)}ä»¶ - ã‚¯ã‚¨ãƒªæœ€é©åŒ–ãŒå¿…è¦")
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªã®åˆ†æ
        quality_issues = [t for t in self.test_results if t.get('issues', [])]
        if quality_issues:
            total_issues = sum(len(t.get('issues', [])) for t in quality_issues)
            suggestions.append(f"ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ: {total_issues}ä»¶ - ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®ç¢ºèªãŒå¿…è¦")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        if self.performance_metrics:
            avg_time = self.performance_metrics[-1].get('average_time', 0)
            if avg_time > 5.0:
                suggestions.append(f"å¹³å‡å®Ÿè¡Œæ™‚é–“ãŒé…ã„: {avg_time:.2f}s - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ ã‚’æ¤œè¨")
        
        if not suggestions:
            suggestions.append("ç¾åœ¨ã®ã¨ã“ã‚é‡å¤§ãªå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        return suggestions
    
    def run_full_test_suite(self) -> Dict[str, Any]:
        """å®Œå…¨ãªãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸ” è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        print("=" * 80)
        
        start_time = time.time()
        
        # ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã®ç”Ÿæˆ
        scenarios = self.create_test_scenarios()
        print(f"ğŸ“ ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª: {len(scenarios)}ä»¶")
        
        # å„ã‚·ãƒŠãƒªã‚ªã®å®Ÿè¡Œ
        for scenario in scenarios:
            test_result = self.run_single_test(scenario)
            self.test_results.append(test_result)
            time.sleep(0.5)  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è² è·è»½æ¸›
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        performance_result = self.run_performance_test()
        self.performance_metrics.append(performance_result)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # çµæœã®é›†è¨ˆ
        successful_tests = sum(1 for t in self.test_results if t.get('success', False))
        failed_tests = len(self.test_results) - successful_tests
        
        # æ”¹å–„ææ¡ˆã®ç”Ÿæˆ
        suggestions = self.generate_improvement_suggestions()
        
        # ç·åˆçµæœ
        summary = {
            'timestamp': datetime.now().isoformat(),
            'total_execution_time': total_time,
            'test_scenarios': len(scenarios),
            'successful_tests': successful_tests,
            'failed_tests': failed_tests,
            'success_rate': (successful_tests / len(self.test_results)) * 100,
            'performance_metrics': performance_result,
            'improvement_suggestions': suggestions,
            'detailed_results': self.test_results
        }
        
        # çµæœã®è¡¨ç¤º
        self.print_summary(summary)
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        self.save_results(summary)
        
        return summary
    
    def print_summary(self, summary: Dict[str, Any]):
        """ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 80)
        print(f"å®Ÿè¡Œæ™‚é–“: {summary['total_execution_time']:.2f}ç§’")
        print(f"ãƒ†ã‚¹ãƒˆä»¶æ•°: {summary['test_scenarios']}ä»¶")
        print(f"æˆåŠŸ: {summary['successful_tests']}ä»¶")
        print(f"å¤±æ•—: {summary['failed_tests']}ä»¶")
        print(f"æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        
        perf = summary['performance_metrics']
        print(f"\nğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        print(f"  å¹³å‡å®Ÿè¡Œæ™‚é–“: {perf['average_time']:.2f}ç§’")
        print(f"  æœ€å¤§å®Ÿè¡Œæ™‚é–“: {perf['max_time']:.2f}ç§’")
        print(f"  æœ€å°å®Ÿè¡Œæ™‚é–“: {perf['min_time']:.2f}ç§’")
        
        print(f"\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
        for i, suggestion in enumerate(summary['improvement_suggestions'], 1):
            print(f"  {i}. {suggestion}")
    
    def save_results(self, summary: Dict[str, Any]):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        results_dir = Path("test_results")
        results_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"test_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜: {results_file}")
    
    def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.searcher.close()


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        test_system = AutonomousTestSystem()
        summary = test_system.run_full_test_suite()
        test_system.close()
        
        # æ”¹å–„ãŒå¿…è¦ãªå ´åˆã¯çµ‚äº†ã‚³ãƒ¼ãƒ‰1
        if summary['failed_tests'] > 0 or summary['success_rate'] < 90:
            exit(1)
        else:
            exit(0)
            
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        exit(1)


if __name__ == "__main__":
    main()